# This repository is **the replication of the paper** - <em>Aligner</em>: Achieving Efficient Alignment through Weak-to-Strong Correction </h1>

Using the techniques mentioned in the paper, we trained *Aligner* based on [Gemma-2B](https://huggingface.co/google/gemma-2b) and successfully improved the performance of [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen1.5-72B-Chat) and [Claude3-Opus](https://www.anthropic.com/news/claude-3-family) on [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/). After being corrected by our *Aligner* model, Qwen-72B-Chat's win rate rose to <span style="color: red;"> **32.41%** </span>, with its responses averaging <span style="color: red;"> **1812** </span> tokens, whereas the win rate of Claude3-Opus was enhanced to <span style="color: red;"> **35.21%** </span>, with an average response length of <span style="color: red;"> **1881** </span> tokens. The original repository: https://aligner2024.github.io.
